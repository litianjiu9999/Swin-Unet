{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "!pwd\n",
    "!nvidia-smi\n",
    "import os\n",
    "path=\"/content/drive/MyDrive/TransUNet_origin\"\n",
    "os.chdir(path)\n",
    "os.listdir(path)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install tensorboard\n",
    "!pip install tensorboardX\n",
    "!pip install ml-collections\n",
    "!pip install medpy\n",
    "!pip install SimpleITK\n",
    "!pip install scipy\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from networks.vision_transformer import SwinUnet as ViT_seg\n",
    "from trainer import trainer_synapse\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root_path', type=str,\n",
    "                    default='./data/Synapse/train_npz', help='root dir for data')\n",
    "parser.add_argument('--dataset', type=str,\n",
    "                    default='Synapse', help='experiment_name')\n",
    "parser.add_argument('--list_dir', type=str,\n",
    "                    default='./lists/lists_Synapse', help='list dir')\n",
    "parser.add_argument('--num_classes', type=int,\n",
    "                    default=2, help='output channel of network')\n",
    "parser.add_argument('--output_dir', type=str, help='output dir')                   \n",
    "parser.add_argument('--max_iterations', type=int,\n",
    "                    default=30000, help='maximum epoch number to train')\n",
    "parser.add_argument('--max_epochs', type=int,\n",
    "                    default=150, help='maximum epoch number to train')\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=24, help='batch_size per gpu')\n",
    "parser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\n",
    "parser.add_argument('--deterministic', type=int,  default=1,\n",
    "                    help='whether use deterministic training')\n",
    "parser.add_argument('--base_lr', type=float,  default=0.01,\n",
    "                    help='segmentation network learning rate')\n",
    "parser.add_argument('--img_size', type=int,\n",
    "                    default=224, help='input patch size of network input')\n",
    "parser.add_argument('--seed', type=int,\n",
    "                    default=1234, help='random seed')\n",
    "parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n",
    "parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n",
    "        default=None,\n",
    "        nargs='+',\n",
    "    )\n",
    "parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n",
    "parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n",
    "                    help='no: no cache, '\n",
    "                            'full: cache all data, '\n",
    "                            'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n",
    "parser.add_argument('--resume', help='resume from checkpoint')\n",
    "parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n",
    "parser.add_argument('--use-checkpoint', action='store_true',\n",
    "                    help=\"whether to use gradient checkpointing to save memory\")\n",
    "parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n",
    "                    help='mixed precision opt level, if O0, no amp is used')\n",
    "parser.add_argument('--tag', help='tag of experiment')\n",
    "parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n",
    "parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "if args.dataset == \"Synapse\":\n",
    "    args.root_path = os.path.join(args.root_path, \"train_npz\")\n",
    "config = get_config(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not args.deterministic:\n",
    "        cudnn.benchmark = True\n",
    "        cudnn.deterministic = False\n",
    "    else:\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    dataset_name = args.dataset\n",
    "    dataset_config = {\n",
    "        'Synapse': {\n",
    "            'root_path': args.root_path,\n",
    "            'list_dir': './lists/lists_Synapse',\n",
    "            'num_classes': 2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if args.batch_size != 24 and args.batch_size % 6 == 0:\n",
    "        args.base_lr *= args.batch_size / 24\n",
    "    args.num_classes = dataset_config[dataset_name]['num_classes']\n",
    "    args.root_path = dataset_config[dataset_name]['root_path']\n",
    "    args.list_dir = dataset_config[dataset_name]['list_dir']\n",
    "\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    net = ViT_seg(config, img_size=args.img_size, num_classes=args.num_classes).cuda()\n",
    "    net.load_from(config)\n",
    "\n",
    "    trainer = {'Synapse': trainer_synapse,}\n",
    "    trainer[dataset_name](args, net, args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
